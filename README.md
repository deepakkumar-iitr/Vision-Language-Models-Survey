# Vision-Language-Models-Survey
This repository is for vision language models survey


# Vision Language Models in Affect/Emotion Recognition
1. Visual and textual prompts for enhancing emotion recognition in video
   [Link][https://arxiv.org/abs/2504.17224]
2. vvv

# Video based Vision Language Models
1. VideoLLaMA 2: Advancing Spatial-Temporal Modeling and Audio Understanding in Video-LLMs
   [!Link][https://arxiv.org/abs/2406.07476]
2. [2024 ACL] Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models
3. [2024 EMNLP]Video-LLaVA: Learning United Visual Representation by Alignment Before Projection
4. [2024 ECCV] Sharegpt4video: Improving video understanding and generation with better captions
5. [2024 CVPR] Vllms provide better context for emotion understanding through common sense reasoning

# Vision Language Models in image to text extraction
1. [2025 May] Argus: Vision-Centric Reasoning with Grounded Chain-of-Thought
2. [2025 May] Zero-Shot Vision Language Encoder Grafting via LLM Surrogates
3. [2025 March] Saling Vision Pre-Training to 4K Resolution
4. [2025 March] Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas
5. [2025] Mini-Monkey: MultiScale adaptive cropping for multimodal large language models
6. [2025] EgoTextVQA: Towards Egocentric scene text aware video question answering
7. [2025] OCEAN-OCR: Towards general OCR application via a vision language model
8. [CVPR2024] Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models
   [Github Link][https://github.com/Yuliang-Liu/Monkey]
9. [2024] Llava-next: A strong zero-shot video understanding model
10. [2024] Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution
11. [2024] InternVL2: How far are we to GPT-4V? Closing the gap to commercial multimodal models with open source suites
12. [2024 ] Text-Monkey: An OCR-free Large multimodal model for understanding documents
13. [2023 EMNLP] UReader: Universal OCR free Visually situated language understanding with multimodal large language model
14. [2023 NeurIPS] LLaVA: Visual instruction tuning
15. [2023 TPAMI] SPTS v2: Single point scene text spotting
16. [2023 ICML] Pix2Struct: Screenshot parsing as pretrained for visual language understanding
17. [2023 AAAI] TrOCR: Transformer based optical character recognition with pretrained models
18. [2022 NeurIPS] Flamingo: a visual language model for few short learning
19. [2022 ECCV] MGP-STR: Multi-granularity prediction for scene text recognition designed for OCR tasks
20. [2022 ACL] Chart QA: A benchmark for question answering about charts with visual and logical reasoning
21. [2022 WACV] Infographic VQA
22. [2021 WACV] DocVQA: A dataset for VQA on document images
23. [2019 CVPR] Text VQA: Towards VQA modeling that can read
24. [2019 ICCV] ST-VQA: Scene text visual question answering
25. [2017] NewsQA: A machine comprehension dataset

# Miscellaneous
1. [2024 ICLR] LLAMA-ADAPTER: Efficient Fine-Tuning of Large Language Models with Zero-Initialized Attention
2. [2023] LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model
